{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    #==========python函数必须先定义后使用=============\n",
    "    \n",
    "    # 创建样本数据\n",
    "    def create_data_set():\n",
    "        \n",
    "        data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "        labels = ['no surfacing', 'flippers']\n",
    "        \n",
    "        return data_set, labels\n",
    "    \n",
    "    # 计算信息熵\n",
    "    def calc_shannon_ent(data_set):\n",
    "        \n",
    "        num = len(data_set)\n",
    "        # 为所有的分类类目创建字典\n",
    "        label_counts = {}\n",
    "        for feat_vec in data_set:\n",
    "            current_label = feat_vec[-1]\n",
    "            if current_label not in label_counts.keys():\n",
    "                label_counts[current_label] = 0\n",
    "            label_counts[current_label] += 1\n",
    "            \n",
    "        # 计算信息熵\n",
    "        shannon_ent = 0.0\n",
    "        for key in label_counts:\n",
    "            prob = float(label_counts[key]) / num\n",
    "            shannon_ent = shannon_ent - prob * log(prob,2)\n",
    "            \n",
    "        return shannon_ent\n",
    "    \n",
    "    # 返回特征值等于value的子数据集，且该数据集不包含列（特征）axis\n",
    "    def split_data_set(data_set, axis, value):\n",
    "        ret_data_set = []\n",
    "        for feat_vec in data_set:\n",
    "            if feat_vec[axis] == value:\n",
    "                reduce_feat_vec = feat_vec[:axis]\n",
    "                reduce_feat_vec.extend(feat_vec[axis + 1:])\n",
    "                ret_data_set.append(reduce_feat_vec)\n",
    "        return ret_data_set\n",
    "    \n",
    "    # 按照最大信息增益划分数据\n",
    "    def choose_best_feature_to_split(data_set):\n",
    "        # 1. 求特征个数\n",
    "        num_feature = len(data_set[0]) - 1\n",
    "        # 2. 求经验熵\n",
    "        base_entropy = calc_shannon_ent(data_set)\n",
    "        # 3. 子集香农熵求和\n",
    "        best_info_gain = 0\n",
    "        best_feature_idx = -1\n",
    "        # 计算某个特征的所有值\n",
    "        for feature_idx in range(num_feature):\n",
    "            feature_val_list = [number[feature_idx] for number in data_set]\n",
    "            # 获取无重复的属性值\n",
    "            unique_feature_val_list = set(feature_val_list)\n",
    "            new_entropy = 0\n",
    "            for feature_val in unique_feature_val_list:\n",
    "                # split_data_set为什么是返回不包含当前值的\n",
    "                # 其实就是通过求减去这个值剩余的，来求得需要值  -- 因为反正最后要求和\n",
    "                sub_data_set = split_data_set(data_set, feature_idx, feature_val)\n",
    "                # p(t)\n",
    "                prob = len(sub_data_set) / float(len(data_set)) \n",
    "                # 对各子集香农熵求和\n",
    "                new_entropy += prob * calc_shannon_ent(sub_data_set) \n",
    "             \n",
    "            # 计算信息增益g(D,A)=H(D)-H(D|A)\n",
    "            info_gain = base_entropy - new_entropy\n",
    "            # 最大信息增益\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_feature_idx = feature_idx\n",
    "                \n",
    "        return best_feature_idx\n",
    "\n",
    "    #  统计每个类别出现的次数，并按大到小排序，返回出现次数最大的类别标签\n",
    "    def majority_cnt(class_list):\n",
    "        class_count = {}\n",
    "        for vote in class_list:\n",
    "            if vote not in class_count.keys():\n",
    "                class_count[vote] = 0\n",
    "            class_count[vote] += 1\n",
    "            \n",
    "        sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(1), reversed=True)\n",
    "        \n",
    "        return sorted_class_count[0][0]\n",
    "    \n",
    "    # 构建决策树\n",
    "    def create_tree(data_set, labels):\n",
    "        # 样本分类信息\n",
    "        class_list = [sample[-1] for sample in data_set]\n",
    "        # 类别相同，停止划分\n",
    "        if class_list.count(class_list[-1]) == len(class_list):\n",
    "            return class_list[-1]\n",
    "        \n",
    "        # 如果长度为1，返回出现次数最多的类被  \n",
    "        # == 有疑问？ -- 这里就是特征为空集的那个条件：即特征为空集的时候，返回分类中，类别多的那个\n",
    "        if len(class_list[0]) == 1:\n",
    "            return majority_cnt((class_list))\n",
    "    \n",
    "        # 按照信息增益最高，选取分类属性特征\n",
    "        # 1. 返回分类的特征的数组索引\n",
    "        best_feature_idx = choose_best_feature_to_split(data_set) \n",
    "        # 2. 该特征的label\n",
    "        best_feat_label = labels[best_feature_idx]\n",
    "        # 3. 构建树的字典 -- 有疑问,代码本身有疑问{best_feat_label: {}}\n",
    "        my_tree = {best_feat_label: {}}\n",
    "        # 4. 从label的list中删除该label，相当于待划分的子标签集\n",
    "        del (labels[best_feature_idx])\n",
    "        \n",
    "        feature_values = [example[best_feature_idx] for example in data_set]\n",
    "        unique_feature_values = set(feature_values)\n",
    "        for feature_value in unique_feature_values:\n",
    "            # 子集合\n",
    "            sub_labels = labels[:]\n",
    "            # 构建数据的子数据集，并进行递归\n",
    "            sub_data_set = split_data_set(data_set, best_feature_idx, feature_value)\n",
    "            my_tree[best_feat_label][feature_value] = create_tree(sub_data_set, sub_labels)\n",
    "        return my_tree\n",
    "          \n",
    "    # 决策树分类\n",
    "    def classify(input_tree, feat_labels, test_vec):\n",
    "        # 1. 获取树的第一特征属性\n",
    "        first_str = list(input_tree.keys())[0]\n",
    "        # 2. 树的分子，子集合dict\n",
    "        second_dict = input_tree[first_str]\n",
    "        # 3. 获取决策树第一层在feat_labels中的位置\n",
    "        feat_index = feat_labels.index(first_str)\n",
    "        \n",
    "        for key in second_dict.keys():\n",
    "            if test_vec[feat_index] == key:\n",
    "                if type(second_dict[key]).__name__ == 'dict':\n",
    "                    class_label = classify(second_dict[key], feat_labels, test_vec)\n",
    "                else:\n",
    "                    class_label = second_dict[key]\n",
    "                    \n",
    "                return class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树： {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
      "（1）不浮出水面可以生存，无脚蹼： no\n",
      "（2）不浮出水面可以生存，有脚蹼： yes\n",
      "（3）不浮出水面可以不能生存，无脚蹼： no\n"
     ]
    }
   ],
   "source": [
    "data_set, labels = create_data_set()\n",
    "decision_tree = create_tree(data_set, labels)\n",
    "print (\"决策树：\", decision_tree)\n",
    "data_set, labels = create_data_set()\n",
    "print (\"（1）不浮出水面可以生存，无脚蹼：\", classify(decision_tree, labels, [1, 0]))\n",
    "print (\"（2）不浮出水面可以生存，有脚蹼：\", classify(decision_tree, labels, [1, 1]))\n",
    "print (\"（3）不浮出水面可以不能生存，无脚蹼：\", classify(decision_tree, labels, [0, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
